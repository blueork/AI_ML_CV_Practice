{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f86bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a46d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|██████████| 13/13 [1:13:10<00:00, 337.76s/it]\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:03<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# using a lightweight diffusion model due to lack of time and computational resources\n",
    "model_id = \"stabilityai/stable-diffusion-2-base\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)\n",
    "pipe.to(\"cpu\")  # Use CPU\n",
    "pipe.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Due to the deadline I was only able to load the model and not test the rest of the code\n",
    "##### The reason is initally I useed another stabe diffusion model and it was too large. I wasted significant amount of time on it\n",
    "##### Then I resorted to this small model. But even this took nearly two hours to load and hence did not leave me time for the rest of \n",
    "##### the code\n",
    "\n",
    "\n",
    "# Step 2: Hook into attention\n",
    "attention_store = {}\n",
    "\n",
    "def save_cross_attention(name):\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple):  # (hidden_states, attention_weights)\n",
    "            attention = output[1]      # (batch, heads, tokens, pixels)\n",
    "            attention_store.setdefault(name, []).append(attention.detach().cpu())\n",
    "    return hook\n",
    "\n",
    "def register_attention_hooks(pipe):\n",
    "    for name, module in pipe.unet.named_modules():\n",
    "        if \"attn2\" in name:\n",
    "            module.register_forward_hook(save_cross_attention(name))\n",
    "\n",
    "register_attention_hooks(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Generate image\n",
    "prompt = \"A dog playing with a ball on the beach\"\n",
    "output = pipe(prompt, num_inference_steps=50, guidance_scale=7.5)\n",
    "image = output.images[0]\n",
    "image.save(\"output.png\")\n",
    "\n",
    "# Step 4: Process attention (pick one layer)\n",
    "def get_token_attention(prompt, layer=\"down_blocks.2.attentions.1.transformer_blocks.0.attn2\"):\n",
    "    tokenizer = pipe.tokenizer\n",
    "    text_input = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(text_input[0])\n",
    "    \n",
    "    attention = torch.cat(attention_store[layer], dim=0)  # (steps, batch, heads, tokens, pixels)\n",
    "    attention = attention.mean(dim=2)  # average heads -> (steps, batch, tokens, pixels)\n",
    "\n",
    "    last_step_attention = attention[-1, 0]  # (tokens, pixels)\n",
    "    return last_step_attention, tokens\n",
    "\n",
    "# Step 5: Visualize heatmap\n",
    "def show_attention_map(image, attn_map, token_index, token_label):\n",
    "    image_np = np.array(image)\n",
    "    h, w, _ = image_np.shape\n",
    "    attn = attn_map[token_index].reshape(16, 16)  # pixel layout\n",
    "    attn = cv2.resize(attn.numpy(), (w, h))\n",
    "    attn = attn / attn.max()\n",
    "\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * attn), cv2.COLORMAP_JET)\n",
    "    overlay = cv2.addWeighted(image_np, 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(f\"Attention for token: {token_label}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Step 6: Pick token and visualize\n",
    "attn_map, tokens = get_token_attention(prompt)\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token in [\"dog\", \"ball\", \"beach\"]:  # You can pick interactively\n",
    "        show_attention_map(image, attn_map, idx, token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kdd_task_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
